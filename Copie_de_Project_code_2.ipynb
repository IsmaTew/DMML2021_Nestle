{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copie de Project code 2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9JGePu64Ml4"
      },
      "source": [
        "#Project : Nestle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrMlTr04TlH"
      },
      "source": [
        "Here are the important libraries that we will use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdiuxCSZ4ZXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63dbe4e-0df3-4011-9c94-534853e1ef63"
      },
      "source": [
        "# Install and update spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Download the french language model\n",
        "!python -m spacy download fr\n",
        "\n",
        "# Libraries:\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'fr' are deprecated. Please use the\n",
            "full pipeline package name 'fr_core_news_sm' instead.\u001b[0m\n",
            "Collecting fr-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.2.0/fr_core_news_sm-3.2.0-py3-none-any.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 463 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMdv9sJ9Z_KT"
      },
      "source": [
        "##Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA2Prjju4l2q"
      },
      "source": [
        "First, we read the Training Data using the download link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "igMYiUZi4kCn",
        "outputId": "9c5ed8f9-f62a-498b-d715-5a1cf26b7fde"
      },
      "source": [
        "# Read the data:\n",
        "\n",
        "# We take the url to doawnload the training data:\n",
        "\n",
        "url = \"https://storage.googleapis.com/kagglesdsdata/competitions/32066/2798629/training_data.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1638871683&Signature=BxIwsdE%2BQaVaP44mJZiajFAd05%2F2RADSA4xIKEqvZsO2ulQbUJL5TIBs5jx88hatetORhWF9annOzgcuLJYV5Do2qAcBUfz1whR60FQHgpjv%2F7X3orM8UPG2w7ZhqC0vd3fRpFWdfI%2FqZnGg6CSySAFth8Yultup9ANQl6EMy59y91dl3uC6OYgmiHHHOfOCrU6LJoez2BCER8l1rY1VIKUwJogQhx%2BNDP7kCdDzsLYmkGzwqAn49sLqulJLw%2F7t7Bm5BJclauqXS8FJzXDWzB4S1ndG43qMnQJpFHP8hc2ndPiQX8q1nZNN8NudQjZJjVK4xr1lEm56vZqGRhYrZA%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining_data.csv\"\n",
        "Training_Data = pd.read_csv(url, delimiter=\",\")\n",
        "Training_Data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           sentence difficulty\n",
              "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
              "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCANeI34D-l0"
      },
      "source": [
        "We get some informations about the Training Data in order to see how good they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRz_PoSZ4vGf",
        "outputId": "5cdd2f70-5cde-4583-f1e8-ed9b90f24a71"
      },
      "source": [
        "Training_Data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4800 entries, 0 to 4799\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   id          4800 non-null   int64 \n",
            " 1   sentence    4800 non-null   object\n",
            " 2   difficulty  4800 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 112.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkaocCH34v7s",
        "outputId": "2c4d14a4-2b95-4e82-c30f-8df68b8199e1"
      },
      "source": [
        "# Base rate: the data-set is balanced\n",
        "Training_Data.difficulty.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1    813\n",
              "C2    807\n",
              "C1    798\n",
              "B1    795\n",
              "A2    795\n",
              "B2    792\n",
              "Name: difficulty, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHYFSLHiT3OU"
      },
      "source": [
        "Then, we read the Test Data using also the download link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "mWD4kLmVUA3A",
        "outputId": "7c779143-c55a-4477-eceb-e5be773eecdf"
      },
      "source": [
        "url = \"https://storage.googleapis.com/kagglesdsdata/competitions/32066/2798629/unlabelled_test_data.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1638891443&Signature=Yq3ALBqeX%2BRm7bWum%2FoP%2FCSO3s4iW8mcJfp07Gm11sUWLdennLm6wKgoeI%2FsCc3grJSl7cj%2B9O1JbP2l7gxwMUMH2o0zysO9zPXZODpQCG73sRgurwfRbJqQ3C%2Fv8zVCJ0D1TKaBC8vH2Mn%2FrhmFUm58RerTIk%2FdFmBEScD6nyLN591GA3gAY04NIYCLH34pRCRQ4uwYvDra%2FxYH7ABvLQ8tOwloYXHBkq%2BkssqXUcVhypJS9USj4p%2FkFWj5GwgVf8Vf3ytj2FXDTf0GBw0IEMT2bVHQRE8T1ZWc8%2BLqmdRMqA%2BwA%2BlTCUyua9FyQNokxrEd3j%2FzBNIXf8ZGD3JTDQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dunlabelled_test_data.csv\"\n",
        "Test_Data = pd.read_csv(url, delimiter=\",\")\n",
        "Test_Data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           sentence\n",
              "0   0  Nous dûmes nous excuser des propos que nous eû...\n",
              "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ..."
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7uoHVp-4422"
      },
      "source": [
        "##Tokening the Data With spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2lDdKJHu4_G4",
        "outputId": "ca3c2d28-a27f-4fd0-f599-84523c2f1f12"
      },
      "source": [
        "# We create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "punctuations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7plvw-tt5Akv"
      },
      "source": [
        "# And a list of stopwords\n",
        "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
        "\n",
        "#list(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdoJTkyO5IdP"
      },
      "source": [
        "# Let's implement the tokenizer function:\n",
        "\n",
        "# Load French language model:\n",
        "sp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# Tokenizer function:\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "    \n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Remove anonymous dates and people\n",
        "    mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
        "    mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"] ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCDGvg0f79Iv"
      },
      "source": [
        "Vectorization Feature Engineering (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeBBr48-5OQj"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exzyB7GJ8j-9"
      },
      "source": [
        "##Classification of the reviews using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4x6t0eQ8k03"
      },
      "source": [
        "# Select features\n",
        "X_train = Training_Data['sentence'] # the features we want to analyze\n",
        "y_train = Training_Data['difficulty'] # the labels, or answers, we want to test against"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpsiNbwD9jXo",
        "outputId": "1c9252ba-0ba5-408f-889c-ae515ca229c2"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Define classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7f217f4f2a70>)),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCV2SVk9HhTZ"
      },
      "source": [
        "Then we do the prediction with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6daWjvNHYEq"
      },
      "source": [
        "X_test = Test_Data[\"sentence\"]\n",
        "y_LogRegPred = pipe.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wQz6dHeKCzD"
      },
      "source": [
        "LogisticReg_pred = pd.DataFrame(y_LogRegPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fztnJSvFeMlz"
      },
      "source": [
        "LogisticReg_pred.columns = ['difficulty']\n",
        "LogisticReg_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ROHFwq2Igis"
      },
      "source": [
        "LogisticReg_pred.to_csv('LogisticReg_prediction.csv', header = True, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqeWpXWdJkEJ"
      },
      "source": [
        "##Classification of the reviews using knn regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mATM6MFBKID1"
      },
      "source": [
        "# Select features\n",
        "X_train = Training_Data['sentence'] # the features we want to analyze\n",
        "y_train = Training_Data['difficulty'] # the labels, or answers, we want to test against"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpFtnOXbKZEw",
        "outputId": "8ad7e8e0-4df6-4abd-ea7a-af63f52f0594"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Define classifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7f217f4f2a70>)),\n",
              "                ('classifier', KNeighborsClassifier(n_neighbors=6))])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9F_p1SFKoEp"
      },
      "source": [
        "X_test = Test_Data['sentence']\n",
        "y_KnnPred = pipe.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47lvjHUKxDU"
      },
      "source": [
        "Knn_pred = pd.DataFrame(y_KnnPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8DtPyo6LFyd"
      },
      "source": [
        "Knn_pred.columns = ['difficulty']\n",
        "Knn_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JglG4ANiLVLz"
      },
      "source": [
        "Knn_pred.to_csv('KNN_prediction.csv', header = True, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVw0lRgtRPAd"
      },
      "source": [
        "##Classification of the reviews using Decision Tree regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFs8cKHnRUce"
      },
      "source": [
        "# Select features\n",
        "X_train = Training_Data['sentence'] # the features we want to analyze\n",
        "y_train = Training_Data['difficulty'] # the labels, or answers, we want to test against"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM7ioZwQRp8M",
        "outputId": "958c83a1-9676-4129-ae31-e729eff5a692"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier(max_depth=7, random_state=72)\n",
        "\n",
        "# Define classifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "('classifier', classifier)])\n",
        "\n",
        "#fit\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7f217f4f2a70>)),\n",
              "                ('classifier', DecisionTreeClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Eyns_kVMfE"
      },
      "source": [
        "X_test = Test_Data['sentence']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cwHK72mRxSZ"
      },
      "source": [
        "y_DecisionTreePred = pipe.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t5J6OSeR5CI"
      },
      "source": [
        "DecisionTree_pred = pd.DataFrame(y_DecisionTreePred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-yWLHQcR9Wa"
      },
      "source": [
        "DecisionTree_pred.columns = ['difficulty']\n",
        "DecisionTree_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Wa7rshSBWG"
      },
      "source": [
        "DecisionTree_pred.to_csv('DecisionTree_prediction.csv', header = True, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mveozz_cf6C0"
      },
      "source": [
        "#To improve the model we try doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCD79mzEgCQ3"
      },
      "source": [
        "# Tokenize data - same tokenizer function as before\n",
        "Training_DataDoc2Vec = Training_Data\n",
        "Test_DataDoc2Vec = Test_Data\n",
        "#%%time\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "training_tagged = Training_DataDoc2Vec.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['sentence']), tags = [r.difficulty]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N6jaU7PNX7i"
      },
      "source": [
        "test_tagged = Test_DataDoc2Vec.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['sentence']), tags = None), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6huPq1sgWmv"
      },
      "source": [
        "# Allows to speed up a bit\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4cuoiligXju"
      },
      "source": [
        "# Define Doc2Vec and build vocabulary\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
        "model_dbow.build_vocab([x for x in training_tagged.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPCDVqDKgeh6"
      },
      "source": [
        "# Train distributed Bag of Word model\n",
        "model_dbow.train(training_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWN7gSXZgj0C"
      },
      "source": [
        "# Select X and y\n",
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=100)) for doc in sents])\n",
        "    return targets, regressors\n",
        "\n",
        "y_train, X_train = vec_for_learning(model_dbow, training_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpKM8l0Ugxh9",
        "outputId": "e6e703b3-fdc9-41be-8448-73a423819868"
      },
      "source": [
        "# Fit model on training set\n",
        "logreg = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6y1F9mJIi1F"
      },
      "source": [
        "# To prepare the test data\n",
        "def vec_for_predicting(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    regressors = [(model.infer_vector(doc.words, steps=100)) for doc in sents]\n",
        "    return regressors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGYz53dmLFdx"
      },
      "source": [
        "X_test = vec_for_predicting(model_dbow, test_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hctO2dBiP6QM"
      },
      "source": [
        "y_doc2vecpred = logreg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqatsXSAQOWg"
      },
      "source": [
        "doc2vec_pred = pd.DataFrame(y_doc2vecpred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChySxMHhQNaa"
      },
      "source": [
        "doc2vec_pred.columns = ['difficulty']\n",
        "doc2vec_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFA6fdqeQPEd"
      },
      "source": [
        "doc2vec_pred.to_csv('doc2vec_prediction.csv', header = True, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrB5c1vrbIIA"
      },
      "source": [
        "#To improve our model, we decide to use One Hot Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UvZVeFZepR6"
      },
      "source": [
        "##Update: I don't think that it is possible to use one hot encoder for the text analysing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wIoqMyPLXse"
      },
      "source": [
        "#Draft :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryun1349-gKf"
      },
      "source": [
        "#from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "# Evaluate the model\n",
        "#def evaluate(true, pred):\n",
        "#    precision = precision_score(true, pred, average='micro')\n",
        "#    recall = recall_score(true, pred, average='micro')\n",
        "#    f1 = f1_score(true, pred, average='micro')\n",
        "#    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "#    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "#    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drq_rRwH-iDt"
      },
      "source": [
        "# Predictions\n",
        "#y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "#evaluate(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}