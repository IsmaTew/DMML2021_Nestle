{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copie de Project code 3",
      "provenance": [],
      "collapsed_sections": [
        "z7uoHVp-4422",
        "UAsO83Fp1hyg",
        "yqeWpXWdJkEJ",
        "VVw0lRgtRPAd",
        "Mveozz_cf6C0",
        "4wIoqMyPLXse"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9JGePu64Ml4"
      },
      "source": [
        "#Project : Nestle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrMlTr04TlH"
      },
      "source": [
        "Here are the important libraries that we will use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdiuxCSZ4ZXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2663031-4c09-47ba-af32-68c8f949dd75"
      },
      "source": [
        "# Install and update spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Download the french language model\n",
        "!python -m spacy download fr\n",
        "\n",
        "# Libraries:\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 6.0 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 21.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 72.2 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'fr' are deprecated. Please use the\n",
            "full pipeline package name 'fr_core_news_sm' instead.\u001b[0m\n",
            "Collecting fr-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.2.0/fr_core_news_sm-3.2.0-py3-none-any.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMdv9sJ9Z_KT"
      },
      "source": [
        "##Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA2Prjju4l2q"
      },
      "source": [
        "First, we read the Training Data using the download link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "igMYiUZi4kCn",
        "outputId": "01e4de04-c989-42b9-8a24-9251f0a4e70f"
      },
      "source": [
        "# Read the data:\n",
        "\n",
        "# We take the url to doawnload the training data:\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/NFroehl/DMML2021_Nestle/74eb14159f5f045d427350358637df31b81ea73b/data/training_data.csv\"\n",
        "Training_Data = pd.read_csv(url, delimiter=\",\")\n",
        "Training_Data.head(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           sentence difficulty\n",
              "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
              "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCANeI34D-l0"
      },
      "source": [
        "We get some informations about the Training Data in order to see how good they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRz_PoSZ4vGf",
        "outputId": "a54210e1-b13f-45c5-e404-de09bb8f6dcc"
      },
      "source": [
        "Training_Data.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4800 entries, 0 to 4799\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   id          4800 non-null   int64 \n",
            " 1   sentence    4800 non-null   object\n",
            " 2   difficulty  4800 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 112.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkaocCH34v7s",
        "outputId": "f71ff3e2-3d2b-4924-cd2b-a7dae7c66940"
      },
      "source": [
        "# Base rate: the data-set is balanced\n",
        "class_weights = Training_Data.difficulty.value_counts()\n",
        "class_weights"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1    813\n",
              "C2    807\n",
              "C1    798\n",
              "B1    795\n",
              "A2    795\n",
              "B2    792\n",
              "Name: difficulty, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHYFSLHiT3OU"
      },
      "source": [
        "Then, we read the Test Data using also the download link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "mWD4kLmVUA3A",
        "outputId": "b725988e-e1b3-427f-9699-ce2b3b64625b"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/NFroehl/DMML2021_Nestle/main/data/unlabelled_test_data.csv\"\n",
        "Test_Data = pd.read_csv(url, delimiter=\",\")\n",
        "Test_Data.head(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           sentence\n",
              "0   0  Nous dûmes nous excuser des propos que nous eû...\n",
              "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7uoHVp-4422"
      },
      "source": [
        "##Tokening the Data With spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lDdKJHu4_G4"
      },
      "source": [
        "# We create a list of punctuation marks\n",
        "# punctuations = string.punctuation\n",
        "#punctuations"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7plvw-tt5Akv"
      },
      "source": [
        "# And a list of stopwords\n",
        "#stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
        "\n",
        "#list(stop_words)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdoJTkyO5IdP"
      },
      "source": [
        "# Let's implement the tokenizer function:\n",
        "\n",
        "# Load French language model:\n",
        "#sp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# Tokenizer function:\n",
        "#def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    #mytokens = sp(sentence)\n",
        "    \n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    #mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    #mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Remove anonymous dates and people\n",
        "    #mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
        "    #mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"] ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    #return mytokens"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCDGvg0f79Iv"
      },
      "source": [
        "Vectorization Feature Engineering (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeBBr48-5OQj"
      },
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exzyB7GJ8j-9"
      },
      "source": [
        "##Classification of the reviews using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4x6t0eQ8k03"
      },
      "source": [
        "# Select features\n",
        "X = Training_Data['sentence'] # the features we want to analyze\n",
        "y = train = Training_Data['difficulty'] # the labels, or answers, we want to test against"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train/test split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/test splitting code\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
      ],
      "metadata": {
        "id": "nZMMsnA-TkK2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define tokenizing function\n",
        "def basic_tokenizer(sentence):\n",
        "\n",
        "    # Define tokenizer \n",
        "    from nltk.tokenize import word_tokenize\n",
        "    \n",
        "    # Tokenize\n",
        "    sentence = word_tokenize(sentence)\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "# Clean X_train as example\n",
        "# X_train.apply(data_cleaner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAMIyqONrhwE",
        "outputId": "0566a83c-c4ba-4e6e-e66e-d1eae65790d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define vectorizer - use above cleaning function\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, tokenizer=basic_tokenizer, ngram_range=(1,1), min_df=3, max_df=0.9)\n"
      ],
      "metadata": {
        "id": "rLb5WSjWta2I"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression without PCA and Standardization"
      ],
      "metadata": {
        "id": "_85tOAk40oPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform X_train and X_test\n",
        "X_train_vec = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_vec = tfidf.transform(X_test).toarray()\n",
        "\n",
        "print(X_train_vec.shape)\n",
        "X_train_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiCWdupzw7iq",
        "outputId": "42599282-a395-43b1-ab44-514613aac2eb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 3202)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "\n",
        "# Define Model\n",
        "pipe = Pipeline([\n",
        "                 ('LR', LogisticRegression())\n",
        "                 ])\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec, y_train)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ST9a--0UC94",
        "outputId": "01aa25c6-ddfb-40d7-ee5a-631bf19c23df"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  11.6849\n",
            "Train Accuracy:  0.7974\n",
            "Test Accuracy:  0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing precision and recall functions and using them on the test set.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# calculating precision and recall\n",
        "test_pred_LR = pipe.predict(X_test_vec)\n",
        "prec = precision_score(y_test, test_pred_LR, average = \"micro\")\n",
        "rec = recall_score(y_test, test_pred_LR, average = \"micro\")\n",
        "f1 = f1_score(y_test, test_pred_LR, average = \"micro\")\n",
        "\n",
        "print(f'\\nPrecision : {prec}\\nRecall: {rec}\\nF1 score:{f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUbl3SaJ8PGx",
        "outputId": "48713731-2d9e-423f-9e54-b393e89249e0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision : 0.45\n",
            "Recall: 0.45\n",
            "F1 score:0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.predict(X_train_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcS_jEyZu3Mb",
        "outputId": "3b6a40f3-65f9-40aa-eec2-abdcb5e13909"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['C2', 'C1', 'A1', ..., 'B1', 'C1', 'B2'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN without PCA and Standardization"
      ],
      "metadata": {
        "id": "w4LAbdAF0xlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform X_train and X_test for KNN\n",
        "X_train_vec_KNN = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_vec_KNN = tfidf.transform(X_test).toarray()\n",
        "print(X_train_vec.shape)\n",
        "X_train_vec_KNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TldQtLuYyRdh",
        "outputId": "ddfd8b7c-8412-40cb-ddfe-4876bbd36dea"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 3202)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without PCA and Standardization\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define Model\n",
        "pipe_knn = Pipeline([\n",
        "                 ('knn', KNeighborsClassifier(50)),\n",
        "                 ])\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe_knn.fit(X_train_vec_KNN, y_train)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe_knn.score(X_train_vec_KNN, y_train), 4))\n",
        "print('Test Accuracy: ', round(pipe_knn.score(X_test_vec_KNN, y_test), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6_EpS5IxFxV",
        "outputId": "4e41d0ca-9511-4000-b4c2-2f5f92d280f2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  0.0158\n",
            "Train Accuracy:  0.4411\n",
            "Test Accuracy:  0.3906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating precision and recall\n",
        "test_pred_KNN = pipe_knn.predict(X_test_vec_KNN)\n",
        "prec_KNN = precision_score(y_test, test_pred_KNN, average = \"micro\")\n",
        "rec_KNN = recall_score(y_test, test_pred_KNN, average = \"micro\")\n",
        "f1_KNN = f1_score(y_test, test_pred_KNN, average = \"micro\")\n",
        "\n",
        "print(f'\\nPrecision : {prec_KNN}\\nRecall: {rec_KNN}\\nF1 score:{f1_KNN}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8xnnIgVDhpY",
        "outputId": "40be305f-b39d-4028-8779-29e9fc272fce"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision : 0.390625\n",
            "Recall: 0.390625\n",
            "F1 score:0.390625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree without PCA and Standardization"
      ],
      "metadata": {
        "id": "siPbEGOM2Nu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree=DecisionTreeClassifier(max_depth=7, random_state=72)"
      ],
      "metadata": {
        "id": "ZhWNRylA2Zkz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform X_train and X_test for Decision Tree\n",
        "X_train_vec_DT = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_vec_DT = tfidf.transform(X_test).toarray()\n",
        "print(X_train_vec.shape)\n",
        "X_train_vec_DT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1xLZTNG29Q-",
        "outputId": "d3381860-9159-4466-cedd-40b8cea5761c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 3202)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Model\n",
        "pipe_dt = Pipeline([\n",
        "                 ('dt', DecisionTreeClassifier(max_depth=7, random_state=72)),\n",
        "                 ])\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe_dt.fit(X_train_vec, y_train)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe_dt.score(X_train_vec_DT, y_train), 4))\n",
        "print('Test Accuracy: ', round(pipe_dt.score(X_test_vec_DT, y_test), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP-4gAEA2pwD",
        "outputId": "6dead7fa-a62b-42d7-ce0a-f3c270d71d8c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  0.4572\n",
            "Train Accuracy:  0.4172\n",
            "Test Accuracy:  0.3552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating precision and recall\n",
        "test_pred_DT = pipe_dt.predict(X_test_vec_DT)\n",
        "prec_DT = precision_score(y_test, test_pred_DT, average = \"micro\").round(2)\n",
        "rec_DT = recall_score(y_test, test_pred_DT, average = \"micro\")\n",
        "f1_DT = f1_score(y_test, test_pred_DT, average = \"micro\")\n",
        "\n",
        "print(f'\\nPrecision : {prec_DT}\\nRecall: {rec_DT}\\nF1 score:{f1_DT}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pjxMgMSE9IL",
        "outputId": "26f0bd87-29f9-4a3a-a818-8508a2636372"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision : 0.36\n",
            "Recall: 0.35520833333333335\n",
            "F1 score:0.35520833333333335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest without PCA and Standardization"
      ],
      "metadata": {
        "id": "Lk7GuIQT4nvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit and transform X_train and X_test for Decision Tree\n",
        "X_train_vec_RF = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_vec_RF = tfidf.transform(X_test).toarray()\n",
        "print(X_train_vec_RF.shape)\n",
        "X_train_vec_RF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZSpStFY4mlF",
        "outputId": "8cf394d8-21f8-421f-e6de-bb37349e6e2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 3202)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "pipe_RF = Pipeline([\n",
        "                 ('RF', RandomForestClassifier(n_estimators=100)),\n",
        "                 ])\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe_RF.fit(X_train_vec_RF, y_train)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe_RF.score(X_train_vec_RF, y_train), 4))\n",
        "print('Test Accuracy: ', round(pipe_RF.score(X_test_vec_RF, y_test), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycA8wjZU6hkK",
        "outputId": "99e1e6ec-0e1a-4655-aae0-9ae69451f348"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  4.2593\n",
            "Train Accuracy:  0.9979\n",
            "Test Accuracy:  0.4292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating precision and recall\n",
        "test_pred_RF = pipe_RF.predict(X_test_vec_RF)\n",
        "prec_RF = precision_score(y_test, test_pred_RF, average = \"micro\")\n",
        "rec_RF = recall_score(y_test, test_pred_RF, average = \"micro\")\n",
        "f1_RF = f1_score(y_test, test_pred_RF, average = \"micro\")\n",
        "\n",
        "print(f'\\nPrecision : {prec_RF}\\nRecall: {rec_RF}\\nF1 score:{f1_RF}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4biUe2PFY82",
        "outputId": "13b03d08-a87f-4e46-f02e-f2206d99d45a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision : 0.42916666666666664\n",
            "Recall: 0.42916666666666664\n",
            "F1 score:0.42916666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistique Regression with cleaniing"
      ],
      "metadata": {
        "id": "UAsO83Fp1hyg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpsiNbwD9jXo"
      },
      "source": [
        "#from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.pipeline import Pipeline\n",
        "# Define classifier\n",
        "#classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "# pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "#                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "#pipe.fit(X_train, y_train)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCV2SVk9HhTZ"
      },
      "source": [
        "Then we do the prediction with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6daWjvNHYEq"
      },
      "source": [
        "#X_test = Test_Data[\"sentence\"]\n",
        "#y_LogRegPred = pipe.predict(X_test)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wQz6dHeKCzD"
      },
      "source": [
        "#LogisticReg_pred = pd.DataFrame(y_LogRegPred)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fztnJSvFeMlz"
      },
      "source": [
        "#LogisticReg_pred.columns = ['difficulty']\n",
        "#LogisticReg_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ROHFwq2Igis"
      },
      "source": [
        "#LogisticReg_pred.to_csv('LogisticReg_prediction.csv', header = True, index = False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqeWpXWdJkEJ"
      },
      "source": [
        "##Classification of the reviews using knn regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mATM6MFBKID1"
      },
      "source": [
        "# Select features\n",
        "#X_train = Training_Data['sentence'] # the features we want to analyze\n",
        "#y_train = Training_Data['difficulty'] # the labels, or answers, we want to test against"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpFtnOXbKZEw"
      },
      "source": [
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "#from sklearn.pipeline import Pipeline\n",
        "# Define classifier\n",
        "#classifier = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "# Create pipeline\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "#                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "#pipe.fit(X_train, y_train)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9F_p1SFKoEp"
      },
      "source": [
        "#X_test = Test_Data['sentence']\n",
        "#y_KnnPred = pipe.predict(X_test)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47lvjHUKxDU"
      },
      "source": [
        "#Knn_pred = pd.DataFrame(y_KnnPred)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8DtPyo6LFyd"
      },
      "source": [
        "#Knn_pred.columns = ['difficulty']\n",
        "#Knn_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JglG4ANiLVLz"
      },
      "source": [
        "#Knn_pred.to_csv('KNN_prediction.csv', header = True, index = False)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVw0lRgtRPAd"
      },
      "source": [
        "##Classification of the reviews using Decision Tree regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFs8cKHnRUce"
      },
      "source": [
        "# Select features\n",
        "#X_train = Training_Data['sentence'] # the features we want to analyze\n",
        "#y_train = Training_Data['difficulty'] # the labels, or answers, we want to test against"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM7ioZwQRp8M"
      },
      "source": [
        "#from sklearn.tree import DecisionTreeClassifier\n",
        "#tree = DecisionTreeClassifier(max_depth=7, random_state=72)\n",
        "\n",
        "# Define classifier\n",
        "#classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Create pipeline\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "#('classifier', classifier)])\n",
        "\n",
        "#fit\n",
        "#pipe.fit(X_train, y_train)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Eyns_kVMfE"
      },
      "source": [
        "#X_test = Test_Data['sentence']"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cwHK72mRxSZ"
      },
      "source": [
        "#y_DecisionTreePred = pipe.predict(X_test)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t5J6OSeR5CI"
      },
      "source": [
        "#DecisionTree_pred = pd.DataFrame(y_DecisionTreePred)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-yWLHQcR9Wa"
      },
      "source": [
        "#DecisionTree_pred.columns = ['difficulty']\n",
        "#DecisionTree_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Wa7rshSBWG"
      },
      "source": [
        "#DecisionTree_pred.to_csv('DecisionTree_prediction.csv', header = True, index = False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mveozz_cf6C0"
      },
      "source": [
        "#To improve the model we try doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCD79mzEgCQ3"
      },
      "source": [
        "# Tokenize data - same tokenizer function as before\n",
        "#Training_DataDoc2Vec = Training_Data\n",
        "#Test_DataDoc2Vec = Test_Data\n",
        "#%%time\n",
        "#from gensim.models.doc2vec import TaggedDocument\n",
        "#training_tagged = Training_DataDoc2Vec.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['sentence']), tags = [r.difficulty]), axis=1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N6jaU7PNX7i"
      },
      "source": [
        "#test_tagged = Test_DataDoc2Vec.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['sentence']), tags = None), axis=1)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6huPq1sgWmv"
      },
      "source": [
        "# Allows to speed up a bit\n",
        "#import multiprocessing\n",
        "#cores = multiprocessing.cpu_count()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4cuoiligXju"
      },
      "source": [
        "# Define Doc2Vec and build vocabulary\n",
        "#from gensim.models import Doc2Vec\n",
        "\n",
        "#model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
        "#model_dbow.build_vocab([x for x in training_tagged.values])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPCDVqDKgeh6"
      },
      "source": [
        "# Train distributed Bag of Word model\n",
        "#model_dbow.train(training_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWN7gSXZgj0C"
      },
      "source": [
        "# Select X and y\n",
        "#def vec_for_learning(model, tagged_docs):\n",
        "    #sents = tagged_docs.values\n",
        "    #targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=100)) for doc in sents])\n",
        "   # return targets, regressors\n",
        "\n",
        "#y_train, X_train = vec_for_learning(model_dbow, training_tagged)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpKM8l0Ugxh9"
      },
      "source": [
        "# Fit model on training set\n",
        "#logreg = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "#logreg.fit(X_train, y_train)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6y1F9mJIi1F"
      },
      "source": [
        "# To prepare the test data\n",
        "#def vec_for_predicting(model, tagged_docs):\n",
        "  #  sents = tagged_docs.values\n",
        "   # regressors = [(model.infer_vector(doc.words, steps=100)) for doc in sents]\n",
        "   # return regressors"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGYz53dmLFdx"
      },
      "source": [
        "#X_test = vec_for_predicting(model_dbow, test_tagged)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hctO2dBiP6QM"
      },
      "source": [
        "#y_doc2vecpred = logreg.predict(X_test)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqatsXSAQOWg"
      },
      "source": [
        "#doc2vec_pred = pd.DataFrame(y_doc2vecpred)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChySxMHhQNaa"
      },
      "source": [
        "##doc2vec_pred.columns = ['difficulty']\n",
        "#doc2vec_pred.insert(0, 'id', [i for i in range(1200)])"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFA6fdqeQPEd"
      },
      "source": [
        "#doc2vec_pred.to_csv('doc2vec_prediction.csv', header = True, index = False)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrB5c1vrbIIA"
      },
      "source": [
        "#To improve our model, we decide to use One Hot Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UvZVeFZepR6"
      },
      "source": [
        "##Update: I don't think that it is possible to use one hot encoder for the text analysing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wIoqMyPLXse"
      },
      "source": [
        "#Draft :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryun1349-gKf"
      },
      "source": [
        "#from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "# Evaluate the model\n",
        "#def evaluate(true, pred):\n",
        "#    precision = precision_score(true, pred, average='micro')\n",
        "#    recall = recall_score(true, pred, average='micro')\n",
        "#    f1 = f1_score(true, pred, average='micro')\n",
        "#    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "#    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "#    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drq_rRwH-iDt"
      },
      "source": [
        "# Predictions\n",
        "#y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "#evaluate(y_test, y_pred)"
      ],
      "execution_count": 60,
      "outputs": []
    }
  ]
}